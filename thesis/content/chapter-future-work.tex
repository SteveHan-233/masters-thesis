\chapter{Future Work}

We spent most of the year building up the infrastructure for conducting humanoid research, but the actual research is just getting started. I plan to spend the summer to continue working on this project. Here are the items currently on my list - besides open-sourcing the interface code and speeding up the simulation. 

We have several hypotheses for why the baseline method isn't performing well, and we plan to conduct experiments to verify them. First, we want to verify if providing depth information instead of stereoscopic images helps with the performance. If so, we could either provide the depth information using the depth camera on the robot or pre-train the Resnet to predict depth information before solving the downstream task. Second, we don't know if the neural network is able to infer ground-truth object positions. If the object positions are known and the WBC error is nonexistent, then the task can be solved trivially. Therefore, if we eliminate the difficulty caused by understanding the positions of objects, then we can see how difficult it is to learn the WBC dynamics. If the performance of the policy improves dramatically when the objects' ground-truth positions are provided, then we can train a module such as SimNet \cite{kollar2021simnet} to provide this information to the policy. Finally, most tasks only require engaging one hand at a time. Since the policy doesn't understand that the other hand is useless, it might overfit to the random motions of the useless hand. We can verify this by manually reducing the loss of the idle hand. 

Learning the WBC's tracking error is one of the main challenges of our hierarchical approach. We can further tune the WBC hyperparameters to reduce the steady-state tracking error. We can also restrict the bounding box to prevent commands from going outside the reachable range. These methods can improve the tracking performance, but they won't completely eliminate the issue. Since we are providing positions and velocities of each joint to the model, it's very possible for the policy to overfit to the WBC's behavior on specific joint configurations during the 200 demonstrations. Since WBC is highly non-linear, it may not be possible to learn its behavior fully using imitation learning. Therefore, we can try to train a neural network to predict the WBC's behavior using a self-supervised approach, where the robot generates random hand motions and measure the WBC's behavior, which could provide sufficient data for generalization. 

There are also many things from related work that we should try. In HoloDex \cite{arunachalam2022holodex}, the authors used self-supervised learning to learn a low-dimensional embedding from high-dimensional images. By using data augmentation, the image encoder can be trained to recognize only the important features of the image. Since our data from the real robot is small in scale, it's very possible for our image encoder to overfit to the lighting in the room and irrelevant objects in the background, which can be solved by data augmentation. Also, we currently have to re-train the encoder for every new experiment, whereas their representation learning approach allows us to re-use the image encoder to save computational resources.
Their k-nearest neighbors implementation for the downstream task is also worth trying as a baseline.

Finally, the recent work on learning locomotion of humanoids \cite{radosavovic2023learning} with RL has numerous implications on our project. First, the fact that their transformer is able to learn the robot kinematics and dynamics purely from past observations means that it can easily learn the whole-body control behavior. We are currently implementing a transformer architecture to replace our RNN. 
Second, we plan to use RL to fine-tune our policy. After all, the policy can only do so well by imitating the human behavior while being completely ignorant of the task objective and environment interactions. The reward functions in their paper - which takes a whole page to enumerate - account for almost all the scenarios that can be dangerous to deploy on the real-robot, which is why they were able to perform sim-to-real. If we optimize our simulation performance and take inspirations from their reward functions, we could harness the power of combining RL and IL, which is a popular research area recently \cite{wang2023dshape}.
