\chapter{Background}

In this chapter, the necessary background information for this work is introduced. Then, existing literature that relates to our goal will be surveyed.

\section{Imitation Learning}

In imitation learning, a task is demonstrated by a human instructor multiple times until a robot can imitate the demonstrated behavior and perform the task autonomously. 
There are many forms of imitation learning.
The simplest form is Behavioral Cloning, where the robot learns to imitate the motion without inferring the actual goal. This becomes a supervised learning problem with the inputs being the observations and the outputs being the actions taken. This simple method is often effective \cite{zhang2018deep}, but it is prone to covariant shift \cite{ross2011reduction}, in which the errors in actions accumulate and the state deviates from the ones shown in demonstration. 
Another class of algorithms is inverse reinforcement learning, in which the agent tries to reverse-engineer the reward function from the demonstrations. 
However, this is usually computationally expensive due to the requirement to run reinforcement learning as an inner-loop.

The specific imitation learning algorithm used in our work is Behaviorial Cloning with Recurrent Neural Networks (RNN) and Gaussian Mixture Models (GMM). RNN maintains information about the past and can be viewed as injecting the inductive bias of sequentiality. Namely, it preserves time translation invariance by using the same neural network weights at each time step \cite{battaglia2018relational}. GMM uses a mixture of Gaussians with different means to approximate a multimodal distribution, such as the multiple ways that a human may do a specific task.

\section{Whole-body Control (WBC)}

Whole-body control is a robotics approach that aims to control the entire robot's body, including its arms, legs, torso, and head, to achieve a desired task.
The specific form we are using is called implicit hierarchical whole-body controller \cite{Ahn2021VersatileLP}, which uses an implicit hierarchy of tasks to allow for different prioritization of control depending on whether the robot is performing locomotion or manipulation. For example, we would like the robot to prioritize foot motion over hand motion when the robot is walking.

Whole-body control can be formulated as finding the optimal joint acceleration $\ddot{q}^*$ and contact forces $f_r^*$ to minimize a loss function: 
$$
\min_{\ddot{q}, f_r}
 \sum_{i=1}^{n} 
  w_i \|J_i\ddot{q} 
   +\dot{J}_i\dot{q}_m
   -\ddot{x}_i^d\|^2
  +w_{f_r} \| f_r^d - f_r\|^2
  +\lambda_q \|\ddot{q}\|^2
  +\lambda_{f_r} \|f_r\|^2
$$
subject to the constraints of robot dynamics, contact, maximum reaction force, joint position limit, and torque limit.
The first term in the loss function represents the task space error, or the deviation of the robot's end-effector position and orientation from the desired values. The second term represents the contact force error. The third term regularizes the joint acceleration to control the trade-off between task performance and joint motion smoothness. The fourth term regularizes the contact force to prevent damage to the robot or the environment.

\section{Related Work}

\subsection{VR for Imitation Learning}

The work that most closely relates to ours is \cite{zhang2018deep}, which also involves using a consumer VR headset to teleoperate a robot. They show that by using only half an hour of demonstrations collected in VR, an imitation learning algorithm can be trained to perform simple manipulation tasks such as grabbing a ball or pushing a block. 
However, their robot has a fixed base, so they can directly use their robot's built-in Jacobian-transpose based controller instead of a whole-body controller. 
Instead of streaming stereoscopic images to the headset, they use Unity to render the colorized point cloud from the robot's 3D camera to allow the user to look around freely in the VR world. 
They argue that since the robot's head has low degrees of freedom and moves slowly, controlling its movement using the headset's movement will cause motion sickness. 
Indeed, we are faced with the same problem. 
However, instead of using a point cloud that could look strange for the demonstrator, we simply disallowed movements of the head.
This is sufficient for our tasks, and it does not seem to impact immersion. 
For learning, they also use a Behavior Cloning algorithm. 
However, they are only controlling one arm of the robot, whereas we are controlling both. 
They are also using the depth image as an input, but we only provide stereoscopic images. 
Finally, we are using an RNN to preserve information from previous time steps, while they only provide the 5 most recent end effector poses and no image history. 

Another work that utilizes VR for robot learning is \cite{arunachalam2022holodex}, which uses the hand-tracking capabilities of the Oculus Quest II headset to teach a robot hand dexterous manipulation skills. They first retarget the human hand joints to a robot hand with 4 fingers, then they use visual self-supervised learning combined with a simple nearest neighbor algorithm to successfully manipulate objects unseen during training. Although they are using a VR headset, they are not using stereoscopic rendering to create depth perception. Instead, they are rendering the robot's camera as a 2D video in Unity. Similarly, there is a senior thesis project teleoperating a small car in VR, and they also use a 2D video displayed through Unity \cite{carvr}. 

\subsection{Demonstration Interface}

There are multiple ways to collect demonstrations for imitation learning in existing literature. First, a teleoperation input device with 6 degrees of freedom such as the SpaceMouse can be used to control the position and orientation of the robot's end effector \cite{zhu2022robosuite}. However, it can be unintuitive for people to translate a 3D motion into the push and turn of a button, especially if there is a need to control 2 arms at once. In addition, since SpaceMouse controls the velocity instead of position, it requires training to perform actions involving precision. 
Second, humans can directly hold the robot to move it in a desired trajectory by applying force \cite{Akgn2012KeyframebasedLF} \cite{Schulman2013LearningFD}. This is called kinesthetic teaching, but it requires the human to come into the frame to control the robot, which becomes a problem when the policy is trained on vision data. To avoid this, we can also build a replica of the robot and move the replica manually, while the main robot follows its trajectory. For example, \cite{aloha} used a low-cost replica of their bi-manipulation robot to collect demonstrations for fine manipulation tasks. To do so, a human demonstrator pushes the end-effectors of the replica robot to backdrive its joints, and the resulting joint positions are issued as commands for the actual robot to follow. While this method achieved impressive results for the fixed-base robot, they cannot handle the floating-base dynamics of humanoid platforms, which requires torque control to account for the dynamics of the robot. 

Since we can manipulate a replica of the robot to create demonstrations, why can't we use our own body as this replica? After all, humanoids are design to mimic the morphology of humans. 
Indeed, we can directly record the kinematics of human motions \cite{Billard:2013}. Using either a camera or a motion capture system, we can measure the angular displacement of the joints precisely, and then we can map the values to the robot's joints. However, robots can have different mass distributions and degrees of freedom than humans. So, the actions that humans perform may be impossible for robots or cause them to lose balance. Therefore, learning a good mapping and using a whole-body controller to maintain balance are crucial to the success of this method.

\subsection{Learning for Humanoids}
As mentioned in the introduction, there does not seem to be prior work on training humanoid robots to perform manipulation tasks. 
Two works that cut close are \cite{4115578}, which uses Hidden Markov models to imitate a human demonstration, and \cite{7989550}, which uses motion planning to produce fluid transitions between locomotion, loco-manipulation, and manipulation. However, both works only consider a simple simulation with only kinematics and no dynamics, so their results are impossible to be applied to the real world.

Nonetheless, there are recent successes on learning humanoid locomotion.
\cite{kumar2022adapting} adapts the Rapid Motor Adaptation work \cite{kumar2021rma} to bipedal robots. It uses an adaptation module to enable the walking controller to adapt to changing environment in real-time. \cite{radosavovic2023learning} uses IsaacGym simulation to train a humanoid locomotion controller that responds to velocity commands robustly. Their work seems to be a more robust version of \cite{kumar2022adapting} where instead of explicitly adapting to the environment, the transformer implicitly infers the environment through past actions rapidly in every time step. Indeed, they showed that their controller can even maintain balance when objects are thrown at it. There have been attempts at using Isaac Gym and RL to train humanoids to walk in simulation in a brute-force manner without success. What this work did different is that they
1. Use a two-step approach to first train an RL policy using ground-truth observations and guidance from gait heuristics, during which they can iterate quickly to find the best reward functions and gait parameters, and then train the real RL policy with actual observations. They use the policy from the first step to guide the real policy by adding the KL divergence between the two policies in the loss function and annealing its weight as training goes on.
2. Use a transformer model that is only given the positions and velocities of the body and joints as well as previous commands, so it’s practically “blind” when it comes to the physical environment. Just by observing the effect of its previous actions on the robot body, the transformer is able to quickly adapt in-context. Although in their experiment the LSTM baseline got close in success rate, the transformer is able to walk much faster and adapt to new situations much more quickly. They claim that the LSTM baseline is not able to transfer to the real robot.
However, they are able to perform zero-shot sim-to-real transfer of the transformer policy since they do aggressive domain randomization from everything in the robot to the environment. Unlike a classical control algorithm, their model on the real robot is able to adapt to a backpack added on its back and even motor malfunctions (simulated by decreasing PD gains for a motor by a half).

\subsection{Humanoid Teleoperation}

There is a recent surge in interest to remotely teleoperate robots. 
XPRIZE hosted the AVATAR competition in 2022, in which teams compete to develop telepresence technology that allows a human to control a robot remotely to complete a set of tasks. 
Their overarching goal is for humans to be able to see, hear, touch, and interact with the world in the body of a robot, while the people themselves can be on the other side of the world.
While most robots in the competition don't have legs, there are a few humanoids, and most teams are using VR technology to create an immersive visual experience.

There is a recent survey on teleoperation of humanoids \cite{darvish2023teleoperation},  some of which competed in AVATAR. The authors also claim that humanoid teleoperation is a new field with high resource demand, so not many labs are working on it. The works they summarized include very complicated motion capture and feedback setups that tracks the movement of users' feet and relays the sense of touch to the users. The survey unifies the teleoperation systems into a framework with 6 components. First, human measurements are taken. Then, the measurements are retargeted to the robot. After a delayed communication channel, the robot executes its controller to produce low-level commands. The robot's sensory input is then retargeted to human feedback, and human teleperception is provided.
This is similar to our framework, but we only have vision feedback and hand trajectory plus locomotion commands as actions. They also mention that streaming the camera images to VR can cause motion sickness during locomotion, but this could be fixed by adopting digital image stabilization techniques.

The cockpit interface for NASA's Valkyrie humanoid is a good example of a sophisticated VR teleoperation system \cite{nasa}. The interface has a complex user interface that allows the operator to directly specify where to place future footsteps. There is a 3D model of the humanoid for visualization, and both a depth point cloud and a 2D RGB footage of the environment are presented to the user. 

These complex teleoperation systems make sense if the goal is to create immersive telepresence or to perform critical missions in outer space. However, if the goal is to teach robots common loco-manipulation skills, these systems can be overkill. In order to scale up data collection and lower the barrier of entry for humanoid research, I opted to use a single Oculus Quest II as the interface. Although it is missing many components such as feet tracking and tactile sensing, it is enough for simple loco-manipulation tasks. In addition, VR is a rapidly developing industry, so future household headsets will mostly likely incorporate more sensors. For example, the recently-released Meta Quest Pro supports leg tracking.
