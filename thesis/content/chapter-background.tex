\chapter{Background}

Now that the motivation of the project has been discussed, I will introduce the formulations used in this work. Then, existing literature that relates to our goal will be surveyed.

\section{Imitation Learning}
\subsection{RNN}

\section{Whole-body Control}

\section{Related Work}

\subsection{VR for Imitation Learning}

The work that most closely relates to ours is \cite{zhang2018deep}, which also uses a consumer VR headset to teleoperate a robot. 
Their robot has a fixed base, so they can directly use their robot's built-in Jacobian-transpose based controller. 
Instead of streaming stereoscopic images to the headset, they use Unity to render the colorized point cloud from the robot's 3D camera to allow the user to look around freely in the VR world. 
They argue that since the robot's head has low degrees of freedom and moves slowly, controlling its movement using the headset's movement will cause motion sickness. 
Indeed, we are faced with the same problem. 
However, instead of using a point cloud that could look strange for the demonstrator, we simply disallowed movements of the head.
This is sufficient for our tasks, and it does not seem to impact immersion. 
For learning, they also use a Behavior Cloning algorithm. 
However, they are only controlling one arm of the robot, whereas we are controlling both. 
They are also using the depth image as an input, but we only provide stereoscopic images. 
Finally, we are using an RNN to preserve information from previous time steps, while they only provide the 5 most recent end effector poses and no image history. 

\cite{arunachalam2022holodex}

\subsection{Humanoid Learning Locomotion}

\cite{}

https://ashish-kmr.github.io/a-rma/

https://humanoid-transformer.github.io

\subsection{Humanoid Teleoperation}

There is a recent surge in interest to remotely teleoperate robots. 
XPRIZE hosted the AVATAR competition, in which teams 



\subsection{Demonstration Interface}

There are multiple ways to collect demonstrations for imitation learning in existing literature. First, a teleoperation input device with 6 degrees of freedom such as the SpaceMouse can be used to control the position and orientation of the robot's end effector \cite{zhu2022robosuite}. However, it can be unintuitive for people to translate a 3D motion into the push and turn of a button, especially if there is a need to control 2 arms at once. In addition, since SpaceMouse controls the velocity instead of position, it requires training to perform actions involving precision. 
Second, humans can directly hold the robot to move it in a desired trajectory by applying force \cite{Akgn2012KeyframebasedLF} \cite{Schulman2013LearningFD}. This is called kinesthetic teaching, but it requires the human to come into the frame to control the robot, which becomes a problem when the policy is trained on vision data. To avoid this, we can also build a replica of the robot and move the replica manually, while the main robot follows its trajectory. For example, \cite{aloha} used a low-cost replica of their bi-manipulation robot to collect demonstrations for fine manipulation tasks. To do so, a human demonstrator pushes the end-effectors of the replica robot to backdrive its joints, and the resulting joint positions are issued as commands for the actual robot to follow. While this method achieved impressive results for the fixed-base robot, they cannot handle the floating-base dynamics of humanoid platforms, which requires torque control to account for the dynamics of the robot. 

Since we can manipulate a replica of the robot to create demonstrations, why can't we use our own body as this replica? After all, humanoids are design to mimic the morphology of humans. 


Indeed, we can directly record the kinematics of human motions \cite{Billard:2013}. Using either a camera or a motion capture system, we can measure the angular displacement of the joints precisely, and then we can map the values to the robot's joints. However, robots can have different mass distributions and degrees of freedom than humans. So, the actions that humans perform may be impossible for robots or cause them to lose balance. 

\subsection{Videos}

https://arxiv.org/abs/2104.07810
There are proposals to massively scale up human demonstration data using YouTube videos. For example, \cite{chang2020semantic} shows that by watching YouTube videos of house tours, an off-policy Q-learning algorithm can learn the semantic cues in a human environment to improve navigation efficiency. To make it possible to learn dexterous manipulation skills from YouTube videos, \cite{sivakumar2022robotic} trained a neural network to retarget human finger poses from a video to a robotic hand. 

Given the rapid development of humanoid hardware, cracking the code of human-like locomotion is the next big challenge. The most common approach is to use a combination of inverse kinematics and inverse dynamics to generate the joint torques that will move the robot to the desired pose. However, this approach is limited by the complexity of the inverse kinematics and inverse dynamics algorithms. Inverse kinematics is a non-convex optimization problem, and the inverse dynamics problem is also non-convex due to the non-linear constraints. This makes it difficult to find the optimal solution, and the solution is often suboptimal.

Human-to-Robot Imitation in the Wild

VideoDex: Learning Dexterity from Internet Videos

Learning to Manipulate Tools by Aligning Simulation to Video Demonstration

Zero-Shot Robot Manipulation from Passive Human Videos]Zero-Shot Robot Manipulation from Passive Human Videos


Learning from Observations Using a Single Video Demonstration and Human Feedback


